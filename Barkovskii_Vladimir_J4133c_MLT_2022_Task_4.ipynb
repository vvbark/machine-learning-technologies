{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c197bec8-1a80-4730-8723-b79925dd6bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Владимир\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Владимир\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "import re, math\n",
    "from operator import methodcaller\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# if it is importatn to use lemm and stop words\n",
    "\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def compute_tf(text):\n",
    "    tf_text = Counter(text)\n",
    "    for i in tf_text:\n",
    "        tf_text[i] = tf_text[i] / float(len(text))\n",
    "    return tf_text\n",
    "\n",
    "def compute_idf(word, corpus):\n",
    "    return math.log10(len(corpus) / sum([1.0 for i in corpus if word in i]))\n",
    "\n",
    "def compute_tfidf(corpus):\n",
    "    \n",
    "    documents_list = []\n",
    "\n",
    "    for text in corpus:\n",
    "        tf_idf_dictionary = {}\n",
    "        computed_tf = compute_tf(text)\n",
    "        for word in computed_tf:\n",
    "            tf_idf_dictionary[word] = computed_tf[word] * compute_idf(word, corpus)\n",
    "        documents_list.append(tf_idf_dictionary)\n",
    "\n",
    "    return documents_list\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f96c24-fb08-4e49-9e12-bc7e1a72e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text:\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath, 'r') as file:\n",
    "            text = file.read()\n",
    "        self.text = repr(text)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.text)\n",
    "    \n",
    "    def lower(self):\n",
    "        self.text = [[word.lower() for word in corpus] for corpus in self.text]\n",
    "    \n",
    "    def replace(self, old, new):\n",
    "        self.text = self.text.replace(old, new)\n",
    "        \n",
    "    def split(self, splitter):\n",
    "        self.text = self.text.split(splitter)\n",
    "                        \n",
    "    def lemmatize(self, lemmatizer):\n",
    "        self.text = [[lemmatizer(w) for w in corpus] for corpus in self.text]\n",
    "        \n",
    "    def delete_stop_words(self, stopwords):\n",
    "        self.text = [[w for w in corpus if w.lower() not in stopwords] for corpus in self.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a4d08-05ad-407b-9e36-ede523a493b3",
   "metadata": {},
   "source": [
    "# Find most popular words in chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f86aaf-08a5-4f1c-a274-dec56eb6f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Text('alice.txt') # read data\n",
    "text.replace(r'\\n', ' ') # replace some useless info\n",
    "text.replace(r'\\u', '') # another one\n",
    "text.text = re.sub(\"[^a-zA-Z ]+\", \"\", text.text) # replace subs\n",
    "\n",
    "text.text = text.text.split(\"CHAPTER\")[13 : ] # split on chapters\n",
    "\n",
    "text.text = list(map(methodcaller(\"split\", \" \"), text.text)) # lest make tokenization\n",
    "text.text = [list(filter(None, corpus)) for corpus in text.text] # and delete some trash\n",
    "text.lower() # make each word in lower case\n",
    "\n",
    "text.delete_stop_words(stopwords) # deleting stop words\n",
    "text.lemmatize(WordNetLemmatizer().lemmatize) # and then lemmatization\n",
    "\n",
    "tf_idf = compute_tfidf(text.text) # lets compute tf-idf metric for each chapter and for each word\n",
    "\n",
    "frame = pd.DataFrame.from_records(tf_idf).fillna(0.) # so, we had to make the result visible. Filling zeros words which dont maches with chapters\n",
    "\n",
    "# alice has zero tf-idf metric because it comes from each chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b4de08-62c6-4b4b-a3e0-e1a03b613fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER 1\n",
      "\t'drink' with 0.0024 tf-idf\n",
      "\t'fell' with 0.0025 tf-idf\n",
      "\t'eat' with 0.0027 tf-idf\n",
      "\t'bottle' with 0.0032 tf-idf\n",
      "\t'poison' with 0.0033 tf-idf\n",
      "\t'rabbithole' with 0.0033 tf-idf\n",
      "\t'candle' with 0.0033 tf-idf\n",
      "\t'dark' with 0.0033 tf-idf\n",
      "\t'key' with 0.0037 tf-idf\n",
      "\t'bat' with 0.0048 tf-idf\n",
      "CHAPTER 2\n",
      "\t'capital' with 0.0023 tf-idf\n",
      "\t'four' with 0.0024 tf-idf\n",
      "\t'cat' with 0.0030 tf-idf\n",
      "\t'dog' with 0.0031 tf-idf\n",
      "\t'fan' with 0.0031 tf-idf\n",
      "\t'glove' with 0.0031 tf-idf\n",
      "\t'mabel' with 0.0043 tf-idf\n",
      "\t'pool' with 0.0048 tf-idf\n",
      "\t'swam' with 0.0054 tf-idf\n",
      "\t'mouse' with 0.0061 tf-idf\n",
      "CHAPTER 3\n",
      "\t'northumbria' with 0.0026 tf-idf\n",
      "\t'bird' with 0.0028 tf-idf\n",
      "\t'tale' with 0.0028 tf-idf\n",
      "\t'caucusrace' with 0.0039 tf-idf\n",
      "\t'dry' with 0.0044 tf-idf\n",
      "\t'thimble' with 0.0053 tf-idf\n",
      "\t'lory' with 0.0057 tf-idf\n",
      "\t'prize' with 0.0079 tf-idf\n",
      "\t'mouse' with 0.0097 tf-idf\n",
      "\t'dodo' with 0.0114 tf-idf\n",
      "CHAPTER 4\n",
      "\t'honour' with 0.0035 tf-idf\n",
      "\t'yer' with 0.0035 tf-idf\n",
      "\t'ann' with 0.0035 tf-idf\n",
      "\t'mary' with 0.0035 tf-idf\n",
      "\t'glove' with 0.0038 tf-idf\n",
      "\t'fan' with 0.0038 tf-idf\n",
      "\t'bottle' with 0.0038 tf-idf\n",
      "\t'bill' with 0.0047 tf-idf\n",
      "\t'puppy' with 0.0062 tf-idf\n",
      "\t'window' with 0.0071 tf-idf\n",
      "CHAPTER 5\n",
      "\t'righthand' with 0.0021 tf-idf\n",
      "\t'green' with 0.0023 tf-idf\n",
      "\t'hookah' with 0.0030 tf-idf\n",
      "\t'size' with 0.0032 tf-idf\n",
      "\t'father' with 0.0038 tf-idf\n",
      "\t'youth' with 0.0062 tf-idf\n",
      "\t'egg' with 0.0062 tf-idf\n",
      "\t'serpent' with 0.0125 tf-idf\n",
      "\t'pigeon' with 0.0125 tf-idf\n",
      "\t'caterpillar' with 0.0151 tf-idf\n",
      "CHAPTER 6\n",
      "\t'grinned' with 0.0027 tf-idf\n",
      "\t'duchess' with 0.0027 tf-idf\n",
      "\t'cook' with 0.0035 tf-idf\n",
      "\t'grunted' with 0.0035 tf-idf\n",
      "\t'wow' with 0.0053 tf-idf\n",
      "\t'mad' with 0.0054 tf-idf\n",
      "\t'pig' with 0.0057 tf-idf\n",
      "\t'baby' with 0.0070 tf-idf\n",
      "\t'cat' with 0.0075 tf-idf\n",
      "\t'footman' with 0.0106 tf-idf\n",
      "CHAPTER 7\n",
      "\t'oclock' with 0.0029 tf-idf\n",
      "\t'civil' with 0.0029 tf-idf\n",
      "\t'asleep' with 0.0032 tf-idf\n",
      "\t'tea' with 0.0038 tf-idf\n",
      "\t'draw' with 0.0042 tf-idf\n",
      "\t'twinkle' with 0.0077 tf-idf\n",
      "\t'hare' with 0.0089 tf-idf\n",
      "\t'march' with 0.0089 tf-idf\n",
      "\t'hatter' with 0.0177 tf-idf\n",
      "\t'dormouse' with 0.0188 tf-idf\n",
      "CHAPTER 8\n",
      "\t'cat' with 0.0036 tf-idf\n",
      "\t'rosetree' with 0.0037 tf-idf\n",
      "\t'soldier' with 0.0046 tf-idf\n",
      "\t'five' with 0.0046 tf-idf\n",
      "\t'procession' with 0.0055 tf-idf\n",
      "\t'executioner' with 0.0055 tf-idf\n",
      "\t'king' with 0.0057 tf-idf\n",
      "\t'gardener' with 0.0073 tf-idf\n",
      "\t'hedgehog' with 0.0092 tf-idf\n",
      "\t'queen' with 0.0095 tf-idf\n",
      "CHAPTER 9\n",
      "\t'sigh' with 0.0021 tf-idf\n",
      "\t'chin' with 0.0022 tf-idf\n",
      "\t'school' with 0.0028 tf-idf\n",
      "\t'tortoise' with 0.0029 tf-idf\n",
      "\t'queen' with 0.0038 tf-idf\n",
      "\t'duchess' with 0.0040 tf-idf\n",
      "\t'gryphon' with 0.0086 tf-idf\n",
      "\t'moral' with 0.0087 tf-idf\n",
      "\t'mock' with 0.0140 tf-idf\n",
      "\t'turtle' with 0.0146 tf-idf\n",
      "CHAPTER 10\n",
      "\t'beautiful' with 0.0071 tf-idf\n",
      "\t'soup' with 0.0071 tf-idf\n",
      "\t'soooop' with 0.0075 tf-idf\n",
      "\t'whiting' with 0.0085 tf-idf\n",
      "\t'join' with 0.0096 tf-idf\n",
      "\t'dance' with 0.0138 tf-idf\n",
      "\t'gryphon' with 0.0146 tf-idf\n",
      "\t'lobster' with 0.0149 tf-idf\n",
      "\t'mock' with 0.0166 tf-idf\n",
      "\t'turtle' with 0.0184 tf-idf\n",
      "CHAPTER 11\n",
      "\t'list' with 0.0036 tf-idf\n",
      "\t'tart' with 0.0043 tf-idf\n",
      "\t'breadandbutter' with 0.0043 tf-idf\n",
      "\t'juror' with 0.0060 tf-idf\n",
      "\t'officer' with 0.0060 tf-idf\n",
      "\t'dormouse' with 0.0112 tf-idf\n",
      "\t'witness' with 0.0120 tf-idf\n",
      "\t'court' with 0.0130 tf-idf\n",
      "\t'king' with 0.0138 tf-idf\n",
      "\t'hatter' with 0.0140 tf-idf\n",
      "CHAPTER 12\n",
      "\t'copy' with 0.0074 tf-idf\n",
      "\t'agreement' with 0.0074 tf-idf\n",
      "\t'copyright' with 0.0078 tf-idf\n",
      "\t'work' with 0.0090 tf-idf\n",
      "\t'e' with 0.0091 tf-idf\n",
      "\t'foundation' with 0.0103 tf-idf\n",
      "\t'electronic' with 0.0111 tf-idf\n",
      "\t'gutenberg' with 0.0115 tf-idf\n",
      "\t'gutenbergtm' with 0.0230 tf-idf\n",
      "\t'project' with 0.0350 tf-idf\n"
     ]
    }
   ],
   "source": [
    "for index, row in frame.iterrows():\n",
    "    print(f\"CHAPTER {index + 1}\")\n",
    "    most_popular_words = row.sort_values().index[-10 : ]\n",
    "    tfidf_values = row.sort_values()[-10 : ]\n",
    "    for word, value in zip(most_popular_words, tfidf_values):\n",
    "        print(f\"\\t'{word}' with {value:.4f} tf-idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f35df1-44cc-4e48-9c35-bff565e776dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER 1 may have 'bat' title with 0.0048 max tf-idf value\n",
      "CHAPTER 2 may have 'mouse' title with 0.0061 max tf-idf value\n",
      "CHAPTER 3 may have 'dodo' title with 0.0114 max tf-idf value\n",
      "CHAPTER 4 may have 'window' title with 0.0071 max tf-idf value\n",
      "CHAPTER 5 may have 'caterpillar' title with 0.0151 max tf-idf value\n",
      "CHAPTER 6 may have 'footman' title with 0.0106 max tf-idf value\n",
      "CHAPTER 7 may have 'dormouse' title with 0.0188 max tf-idf value\n",
      "CHAPTER 8 may have 'queen' title with 0.0095 max tf-idf value\n",
      "CHAPTER 9 may have 'turtle' title with 0.0146 max tf-idf value\n",
      "CHAPTER 10 may have 'turtle' title with 0.0184 max tf-idf value\n",
      "CHAPTER 11 may have 'hatter' title with 0.0140 max tf-idf value\n",
      "CHAPTER 12 may have 'project' title with 0.0350 max tf-idf value\n"
     ]
    }
   ],
   "source": [
    "for index, row in frame.iterrows():\n",
    "    print(f\"CHAPTER {index + 1} may have '{row.sort_values().index[-1]}' title with {row.sort_values()[-1]:.4f} max tf-idf value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19409f25-89d0-489f-b8bd-59ad78f8a141",
   "metadata": {},
   "source": [
    "# Find sentences with Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ee6f770-ce4a-41b0-8422-0a1944a436f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Text('alice.txt') # read data\n",
    "text.replace(r'\\n', ' ') # replace some useless info\n",
    "text.replace(r'\\u', '') # another one\n",
    "text.text = re.sub(\"[^a-zA-Z.' ]+\", \"\", text.text) # replace subs\n",
    "\n",
    "text.text = text.text.split('.') # split on sentences\n",
    "# text.text = text.text.split(\"CHAPTER\")[13 : ] # split on chapters\n",
    "\n",
    "text.text = list(map(methodcaller(\"split\", \" \"), text.text)) # lest make tokenization\n",
    "text.text = [list(filter(None, corpus)) for corpus in text.text] # and delete some trash as empty objects of str\n",
    "text.lower() # make each word in lower case\n",
    "\n",
    "text.text = [sentence for sentence in text.text if 'alice' in sentence] # filter only those sentences which consist alice word\n",
    "\n",
    "text.delete_stop_words(stopwords) # deleting stop words\n",
    "text.lemmatize(WordNetLemmatizer().lemmatize) # and then lemmatization\n",
    "text.delete_stop_words(stopwords) # deleting stop words\n",
    "\n",
    "# filter by verbes\n",
    "corpus = []\n",
    "for sentence in text.text:\n",
    "    corpus.append([])\n",
    "    for w in sentence:\n",
    "        try:\n",
    "            syn = wn.synsets(w)[0].pos()\n",
    "            if syn == 'v': corpus[-1].append(w)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "text.text = corpus\n",
    "# text.text = [[w for w in sentence if  == 'v'] for sentence in text.text]\n",
    "\n",
    "tf_idf = compute_tfidf(text.text) # lets compute tf-idf metric for each chapter and for each word\n",
    "\n",
    "frame = pd.DataFrame.from_records(tf_idf).fillna(0.) # so, we had to make the result visible. Filling zeros words which dont maches with chapters\n",
    "\n",
    "# alice has zero tf-idf metric because it comes from each chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce103513-5fe0-408b-8303-18490d4aa364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'asked' verb have 5.56 tf-idf\n",
      "'made' verb have 5.61 tf-idf\n",
      "'remarked' verb have 5.85 tf-idf\n",
      "'heard' verb have 7.13 tf-idf\n",
      "'got' verb have 7.83 tf-idf\n",
      "'began' verb have 8.31 tf-idf\n",
      "'replied' verb have 9.76 tf-idf\n",
      "'went' verb have 10.29 tf-idf\n",
      "'looked' verb have 12.05 tf-idf\n",
      "'said' verb have 27.81 tf-idf\n"
     ]
    }
   ],
   "source": [
    "for word, tf_idf in frame.sum().sort_values()[-10:].iteritems():\n",
    "    print(f\"'{word}' verb have {tf_idf:.2f} tf-idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d36b7-d771-44c8-8a23-455b9fd6ebc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e72e6d-5c22-4fcf-a176-46ece8fb926d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
